# Run inferencing

This repo contains an example of how to launch multiple vllm servers, one per node.

See the readme file in vllm-0.6.6-post2 directory.
